{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic: Machine Learning From Disaster\n",
    "### by Sung Ahn and Abdul Saleh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## Introduction\n",
    "In this project, we use random forests and (gradient boosting) machine learning algorithms to predict who survived the sinking of the RMS Titanic. On our journey to achieving this goal, we go through the whole data science process from understanding the problem and getting the data to fine-tuning our models and visualizing our results. \n",
    "\n",
    "The Titanic dataset is perhaps the most widely analyzed dataset of all time. There exists a wealth of incredible tutorials online exploring different approaches to analyzing this dataset. So in our own analysis, we draw on the experiences of the huge community of amazing people who have already attempted this problem and shared their conclusions online. We would especially like to thank [Manav Sehgal](https://www.kaggle.com/startupsci/titanic-data-science-solutions), [Jeff Delaney](https://www.kaggle.com/jeffd23/scikit-learn-ml-from-start-to-finish), and [Ahmed Besbes](https://ahmedbesbes.com/how-to-score-08134-in-titanic-kaggle-challenge.html) whom without their insight this project would not have become a reality.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## Outline \n",
    "1. Understanding the problem\n",
    "2. Getting the data\n",
    "3. Exploring the data \n",
    "4. Picking a machine learning algorithm \n",
    "5. Preparing the data for machine learning algorithms\n",
    "6. Training algorithm and fine-tuning model\n",
    "7. Visualizing results and presenting solution \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## Understanding the problem\n",
    "Before we dive into the data analysis and algorithms, we first ask ourselves: is this even a problem that can be solved with machine learning? <br>\n",
    "Luckily for us, lots of books have been written about the sinking of the Titanic. So before we look at the data, we do some background reading and discover that some patterns might exist, most notably: \n",
    "\n",
    "1. Women and children generally got first priority on the life boats \n",
    "    - This tells us that we should look for \n",
    "2. \n",
    "    -\n",
    "3. There was a lot of confusion during the sinking of the ship and people chose to stay on the Titanic for arbitrary reasons. \n",
    "    - There are definitley anomalies in this dataset because it is clear that some people \n",
    "    through we conclused th\n",
    "\n",
    "Aha, so it seems like a there are some patterns that can help us figure out who survived and who didn't. This looks like a great machine learning problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<hr>\n",
    "## Getting the data\n",
    "Kaggle, a platform for data science competitions, has kindly compiled a dataset that is perfect for our needs and put it on their [website](https://www.kaggle.com/c/titanic/data) for budding machine learning enthusiasts to use. The training dataset tells us who survived and who didn't so we can use it to train our model. The test set doesn't tell us the fate of the passengers - that's what we're supposed to predict!\n",
    "\n",
    "After downloading the datasets, we import the data and a few libraries that we will use later on.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fancyimpute import KNN\n",
    "from math import ceil\n",
    "\n",
    "# for visualizations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# for machine learning\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# import data \n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_data.head())\n",
    "print('_'*125)\n",
    "display(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do **Pclass**, **SibSp**, and **Parch** mean? <br>\n",
    "According to Kaggle, **Pclass** = passenger class, **Sibsp** = # of siblings/spouses aboard the Titanic, **Parch** = # of parents/children aboard the Titanic. \n",
    "<br>\n",
    "\n",
    "#### What are the important feature types? \n",
    "- Categorical features: **Survived, Sex, Embarked, Pclass**\n",
    "- Numerical features:\n",
    "  - Discrete: **SibSp, Parch**\n",
    "  - Continuous : **Fare, Age**\n",
    "- Alphanumeric features: **Cabin, Ticket**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find out size of data\n",
    "print(\"training data dimensions:\", train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the data types? Are there missing values?\n",
    "train_data.info()\n",
    "print('_'*125)\n",
    "test_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the missing values? \n",
    "- From training set:\n",
    "    - 687 missing **Cabin** values\n",
    "    - 177 missing **Age** values\n",
    "    - 2 missing **Embarked** values\n",
    "- From test set:\n",
    "    - 327 missing **Cabin** values\n",
    "    - 86 missing **Age** values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize integer and float type features\n",
    "train_data.describe()\n",
    "# Show more details about specific features\n",
    "#data_train.describe(percentiles=[], include=[\"Pclass\"])\n",
    "#data_train.describe(percentiles=[], include=[\"Pclass\"])\n",
    "#data_train.describe(percentiles=[], include=[\"Pclass\"])\n",
    "#data_train.describe(percentiles=[], include=[\"Pclass\"])\n",
    "#data_train.describe(percentiles=[], include=[\"Pclass\"])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What to numerical features tell us?\n",
    "- The sample's survival rate is ~38% which is a bit higher than the actual 32%\n",
    "- Most passengers on board were in 3rd class, while less that 25% where in 1st class\n",
    "- More than 75% of passengers were less that 38 years old and the mean age was 30 \n",
    "- More than 75% of passengers did not travel with their kids or their parents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize object type features\n",
    "train_data.describe(include=[\"O\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What this tells us\n",
    "\n",
    "Hmm, it looks like we have some missing em ages and lots of missing cabin numbers. We will have to figure out ways to deal with those later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## Picking an algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<hr>\n",
    "## Preparing the data for machine learning algorithms\n",
    "First we start by combining the training dataset and the test dataset so that we can edit them both together and ensure they end up in the same format. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the survival data\n",
    "targets = train_data.Survived\n",
    "\n",
    "# Drop survival data so training set and test set have the same shape and can be combined\n",
    "train_data_dropped = train_data.drop([\"Survived\"], 1)\n",
    "\n",
    "# Combine train and test data\n",
    "combined_data = train_data_dropped.append(test_data)\n",
    "\n",
    "combined_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data had 891 entries and the test data had 418 entries. $418 + 891 = 1309$ entires, so this looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting passenger titles: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look back at the data you will notice that passenger titles are always preceded by a comma and followed by a period. So we can create a function that splits the Name value at the comma and at the period to get the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract titles from names and place them in a new column\n",
    "combined_data[\"Title\"] = combined_data[\"Name\"].map(lambda name: name.split(\",\")[1].split(\".\")[0].strip())\n",
    "\n",
    "# Drop names column because we no longer need it\n",
    "combined_data.drop([\"Name\"], 1, inplace=True)\n",
    "\n",
    "# Show all different values in the Title column\n",
    "combined_data.Title.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#back up copy\n",
    "back_up2 = combined_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it worked!\n",
    "<br>\n",
    "We can also group similar titles together to simplify our model and reduce the risk of \"overfitting.\" Overfitting happens when a model is too complex in such a way that it is so good at predicting training data outcomes but not so good at predicting outcomes for unseen data. For example, it would be nice if our model knows how to predict outcomes for countesses, but chances are, there aren't that many countesses on the Titanic, so it would be better to group Royalties  together so that our model generalizes better when looking at new data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title_dict = {\n",
    "    \"Mr\":\"Mr\",\n",
    "    \"Miss\":\"Ms\",\n",
    "    \"Mrs\":\"Mrs\",\n",
    "    \"Master\":\"Master\",\n",
    "    \"Dr\":\"Other\",\n",
    "    \"Rev\":\"Other\",\n",
    "    \"Col\":\"Military\",\n",
    "    \"Ms\":\"Ms\",\n",
    "    \"Mlle\":\"Ms\",\n",
    "    \"Major\":\"Military\",\n",
    "    \"Don\":\"Royal or Noble\",\n",
    "    \"the Countess\":\"Royal or Noble\",\n",
    "    \"Lady\":\"Royal or Noble\",\n",
    "    \"Donna\":\"Royal or Noble\",\n",
    "    \"Sir\":\"Royal or Noble\",\n",
    "    \"Mme\":\"Mrs\",\n",
    "    \"Jonkheer\":\"Royal or Noble\",\n",
    "    \"Capt\":\"Other\"\n",
    "}\n",
    "\n",
    "# Group titles using mappings in dictionary above\n",
    "combined_data[\"Title\"] = combined_data[\"Title\"].map(Title_dict) \n",
    "\n",
    "combined_data[\"Title\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing passenger ages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you recall from the data exploration step, there were about 177 and 86 missing age values from the training set and the data set respectively. We know that age is an important factor in determining survival so we need to come up with a way to fill in the missing ages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to group people by their gender, class, and title and then use these groupings to determine the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select train data and group by Sex, class, and title in that order\n",
    "grouped_train_data = combined_data.head(891).groupby([\"Sex\",\"Pclass\", \"Title\"])\n",
    "\n",
    "# Select test data and group by Sex, class, and title in that order\n",
    "grouped_test_data = combined_data.iloc[891:].groupby([\"Sex\", \"Pclass\", \"Title\"])\n",
    "\n",
    "# Find and display medians \n",
    "display(grouped_train_data.median())\n",
    "print('_'*125)\n",
    "display(grouped_test_data.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the function we want to create to fill in the missing ages first checks the passenger's age, then their class, then their title and uses that info to determine what age to give them. If we haven't seen that title before, we should just plug in the median age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-warning\">Note that we have to be super careful not to introduce any information from the test data into the training data. The point of a predictive machine learning model is to make accurate predictions about new data that is *unseen* during the training.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to round approximations of missing ages to nearest 0.5             \n",
    "def round_age(age):\n",
    "    return round(age * 2) / 2\n",
    "\n",
    "\n",
    "# Function that fills in missing ages\n",
    "def age_filler(incomplete_data, grouped_data, Training=True, size=0):\n",
    "    # Change index to PassengerId so that each passenger has a unique index\n",
    "    if Training != True:\n",
    "        incomplete_data.reset_index()   \n",
    "    \n",
    "    for row in grouped_data:\n",
    "        try:\n",
    "            # This is a data frame the has passengers sharing the same gender, class, and title\n",
    "            subset_incomplete_data = incomplete_data.loc[(incomplete_data[\"Sex\"]==row[0][0]) \n",
    "                                                        & (incomplete_data[\"Pclass\"]==row[0][1]) \n",
    "                                                        & (incomplete_data[\"Title\"]==row[0][2])]\n",
    "        # Skip to next row if no values are found\n",
    "        except KeyError:\n",
    "            continue\n",
    "            \n",
    "        # Extract age and fare columns\n",
    "        subset_incomplete_data = subset_incomplete_data[[\"Age\", \"Fare\"]]\n",
    "\n",
    "        # Array of global indexes of passengers in subset with missing ages\n",
    "        missing_ages_global_index = subset_incomplete_data.index[subset_incomplete_data[\"Age\"].isnull()].tolist()\n",
    "        missing_ages_global_index.sort()\n",
    "\n",
    "        # Array of local indexes of passengers in subset with missing ages\n",
    "        missing_ages_local_index = np.where(subset_incomplete_data[\"Age\"].isnull())[0]\n",
    "        missing_ages_local_index.sort()\n",
    "\n",
    "        # Get number of passengers in subset_train_data, use this number to calculate number of KNN neighbours \n",
    "        subset_incomplete_data_size = subset_incomplete_data.shape[0]\n",
    "        \n",
    "        try:\n",
    "            # Use KNN to fill in missing ages based on similarities between passegers' fares\n",
    "            # Returns an unindexed but ordered numpy array of ages\n",
    "            subset_complete_data = KNN(k=ceil(subset_incomplete_data_size*0.05)).complete(subset_incomplete_data)\n",
    "        # Handles case when there are no missing values\n",
    "        except ValueError:\n",
    "            continue\n",
    "            \n",
    "        counter = 0\n",
    "        # Iterate over estimated ages in subset and place them back in the original dataset\n",
    "        # Use indexes to match values from subset to original data set\n",
    "        for passenger_local_index in missing_ages_local_index:\n",
    "            passenger_estimated_age = round_age(subset_complete_data[passenger_local_index][0])\n",
    "            passenger_global_index = missing_ages_global_index[counter]\n",
    "            counter+=1\n",
    "            completed_data = incomplete_data.set_value(passenger_global_index, \"Age\", passenger_estimated_age)\n",
    "    # Handles test set\n",
    "    if Training != True:\n",
    "        return completed_data.iloc[size:] \n",
    "    # Handles training set\n",
    "    else:\n",
    "        return completed_data\n",
    "\n",
    "train_data = combined_data.head(891).copy()\n",
    "\n",
    "# Estimate missing ages in training set\n",
    "train_data_filled = age_filler(train_data, grouped_train_data)\n",
    "\n",
    "# Estimate missing ages in test set\n",
    "test_data_filled = age_filler(combined_data.copy(), grouped_train_data, Training = False, size = 891)\n",
    "\n",
    "# Combining training and test set, now with estimates for all ages\n",
    "combined_data = train_data_filled.append(test_data_filled).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = train_data_filled.append(test_data_filled).reset_index()\n",
    "#combined_data = combined_data.reset_index()\n",
    "display(combined_data[888:893])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Fare and Embarked features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this data set we have one missing fare value and two missing Embarked values, so let's just fill them in directly.\n",
    "<br><br>\n",
    "First: let's look at the passenger with a missing fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_fare_index = combined_data.index[combined_data[\"Fare\"].isnull()]\n",
    "combined_data.iloc[missing_fare_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This passenger is a male in 3rd class whose title is Mr.\n",
    "Let's find the subset of passengers who also have the same gender, class, and title as this passenger and then assign him the median fare of that subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = combined_data.loc[(combined_data[\"Sex\"]==\"male\")\n",
    "                          & (combined_data[\"Pclass\"]==3)\n",
    "                          & (combined_data[\"Title\"]==\"Mr\")]\n",
    "\n",
    "# Fill empty fare with median\n",
    "combined_data[\"Fare\"].fillna(float(subset[\"Fare\"].median()), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do a similar thing for the missing Embarked values, but we replace missing values with the mode instead.\n",
    "We also make sure we do not leak data from the test set to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_embarked_index = combined_data.index[combined_data[\"Embarked\"].isnull()]\n",
    "missing_embarked_rows = combined_data.iloc[missing_embarked_index]\n",
    "display(missing_embarked_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in missing_embarked_rows.iterrows():\n",
    "    subset = combined_data.head(891).loc[(combined_data.head(891)[\"Sex\"]==row[\"Sex\"])\n",
    "                                        & (combined_data.head(891)[\"Pclass\"]==row[\"Pclass\"])\n",
    "                                        & (combined_data.head(891)[\"Title\"]==row[\"Title\"])]\n",
    "    combined_data.set_value(index, \"Embarked\", str(subset[\"Embarked\"].mode());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fare and Emarked now have no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Family_Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create the **FamilySize** feature by adding **SibSp** and **Parch** and 1. This is makes sense because a **FamilySize** feature might be a better predictor than either of the other features separatley. Families tend to stick together, which probably affected their chanced of survival. We then drop the **SibSp** and **Parch** columns because the information the contain are summarized by the new **FamilySize** feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data[\"Family_Size\"] = combined_data[\"SibSp\"] + combined_data[\"Parch\"] + 1\n",
    "\n",
    "combined_data_dropped = combined_data.drop([\"SibSp\", \"Parch\", \"Cabin\", \"Ticket\", \"PassengerId\"], 1)\n",
    "\n",
    "train_set_final = combined_data_dropped.head(891).copy().reindex()\n",
    "display(train_set_final.head())\n",
    "test_set_final = combined_data_dropped.iloc[891:].copy().reset_index().drop([\"index\"], 1)\n",
    "display(test_set_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the data we are working with into H2O dataframes so that the H2O algorithms can work with them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o_train_data = h2o.H2OFrame(train_set_final)\n",
    "h2o_test_data = h2o.H2OFrame(test_set_final)\n",
    "\n",
    "# This is the survival data we put aside earlier\n",
    "targets_frame = targets.to_frame();\n",
    "h2o_targets = h2o.H2OFrame(targets_frame)\n",
    "\n",
    "h2o_targets.shape\n",
    "h2o_train_data.shape\n",
    "# Combine survival data and training data again\n",
    "#h2o_train_data_survival = h2o_train_data.cbind(h2o_targets)\n",
    "#display(h2o_train_data_survival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o_train_data.describe()\n",
    "#display(h2o_test_data)\n",
    "#display(targets_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators.random_forest import H2ORandomForestEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = H2ORandomForestEstimator(ntrees=200, nfolds=6, seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF.train(h2o_train_data, h2o_targets, training_frame=train,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(targets))\n",
    "#targets = targets.to_frame().reset_index()\n",
    "#targets = targets.rename(columns={0:'list'})\n",
    "#targets.index.name = 'index'\n",
    "#display(targets)\n",
    "train_data_surv = pd.read_csv(\"train.csv\")\n",
    "targets = train_data_surv.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#targets.index.name = 'index'\n",
    "#targets.reset_index()\n",
    "#targets.to_frame()\n",
    "#display(targets)\n",
    "targets = targets.drop([\"index\"], 1, inplace = True)\n",
    "display(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o_targets = h2o.H2OFrame(targets_frame)\n",
    "display(h2o_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
